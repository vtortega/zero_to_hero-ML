# Zero To Hero ML

Code from following Andrej Karpathy excellent lectures on YouTube

### First Lecture

The first lecture goes into the basics of backpropagation, passing through topics like the chain Rule, derivatives and it's impact on the loss function, to arrive at a basic Neural Network built from the basics to really understand it. Constructing Neurons, Layers and then a multilayer Perceptron, that, with the loss function defined, will be inputed data and have its weights changed via Gradient Descent.

It glances into overfitting and the dangers of taking too big or too small steps on the gradient.

A vizualisation with a graph is built too, to help understand back propagation, this together with building the backward function for each operation gives a solid understanding of this very fundamental topic in Machine Learning.
